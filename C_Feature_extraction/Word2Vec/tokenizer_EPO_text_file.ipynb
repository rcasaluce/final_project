{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import inflect\n",
    "import os\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from os.path import expanduser as ospath\n",
    "\n",
    "\n",
    "\n",
    "def open_and_save_text_file():\n",
    "    \n",
    "    path = ospath('~/code_final_project/A_Data_Collection/EPO/Sample_Bulk_full_text_EPO/')\n",
    "    files = os.listdir(path)\n",
    "    #opens all the files in the directory\n",
    "    for file in files:\n",
    "        with open(ospath('~/code_final_project/A_Data_Collection/EPO/Sample_Bulk_full_text_EPO/' + file), encoding=\"utf8\") as f:\n",
    "            data = f.readlines()\n",
    "            name = file[2:-4]\n",
    "            list_text  = select_english_text(data)\n",
    "            abstracts, claims = pre_processing_text_w2v(list_text)\n",
    "            text_doc(abstracts, claims, name)\n",
    "\n",
    "def select_english_text(data):\n",
    "    \n",
    "    \"\"\"from English patents selects only abstracts and claims \n",
    "    \n",
    "    input = data text from the XML text file\n",
    "    output = a list of both claims and abstracts\"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    list_text = []\n",
    "    for patent in data:\n",
    "        patent = re.sub('\\s+',' ',str(patent))#strip the text from tab an new line symbols\n",
    "        language, types, text, type_A_B, pat_num = patent[25:27], patent[28:33], patent[34:], patent[11:13], patent[3:10]\n",
    "        if (language == 'en' and types != 'TITLE' and \n",
    "            types != 'DESCR' and types != 'PDFEP' and \n",
    "            types != 'SRPRT' and types != 'AMEND'):\n",
    "            list_text.append([pat_num, type_A_B, language, types, text])# pat_num,kind, language, type and text\n",
    "    \n",
    "    return list_text\n",
    "\n",
    "   \n",
    "\n",
    "def pre_processing_text_w2v(list_text):\n",
    "    \n",
    "    \"\"\"It processes the text\n",
    "    First, it stripes the text by the last tags and coverts each doc in lower case\n",
    "    Second, it eliminates the numbers in the text and converts the verbs to their root and it eliminates the stop_words\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    input = list of the text from both claims and abstracts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ouptup = abstractss and claims pre-processed\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+(?:[-\\\\]\\w+)?')#keeps words with hyphen and words back slash\n",
    "\n",
    "    list_num = str(list((range(10))))\n",
    "    \n",
    "    abstracts, claims, = ([] for i in range(2))\n",
    "\n",
    "    \n",
    "    for i in range(len(list_text)):\n",
    "        \n",
    "        #parses abstracts\n",
    "        if list_text[i][3] == 'ABSTR':\n",
    "            abstr = list_text[i][4:]\n",
    "            abstr = re.sub(r'(<.*?>)|(-->)', \" \", str(abstr)).lower()#clean the text\n",
    "            abstr = tokenizer.tokenize(str(abstr))\n",
    "            \n",
    "            #at word level\n",
    "            for ii in range(len(abstr)):# to covert num to word\n",
    "                if len(abstr[ii]) != 0:\n",
    "                    if abstr[ii][0] in list_num or abstr[ii] in stop_words:\n",
    "                        abstr[ii] = ''\n",
    "                    else:\n",
    "                        abstr[ii] = word_lemmatizer.lemmatize(abstr[ii], pos=\"v\")\n",
    "            abstracts.append(abstr)\n",
    "\n",
    "        #parses claims\n",
    "        if list_text[i][3] == 'CLAIM':#makes sure to take the claim from type Bs\n",
    "            temp_claim = []\n",
    "            cl = list_text[i][4:]\n",
    "            \n",
    "            list_num_claims = re.findall(r'num=\"([0-9]+)\"', str(cl))#finds tags with claim num\n",
    "        \n",
    "            lenght_list_num_claims = len(list_num_claims) -1\n",
    "            \n",
    "            #claims at patent level\n",
    "            for i in range(len(list_num_claims)):\n",
    "                if lenght_list_num_claims >=1:\n",
    "                    claim = re.findall(r''+ list_num_claims[i]+'(.*?)'+ list_num_claims[i+1], str(cl))\n",
    "                    temp_claim.append(claim)\n",
    "                    lenght_list_num_claims -=1\n",
    "                else:\n",
    "                    claim = re.findall(r''+ list_num_claims[i]+'(.*?)$', str(cl))\n",
    "                    temp_claim.append(claim)\n",
    "                    lenght_list_num_claims -=1\n",
    "            \n",
    "            #at claim level\n",
    "            for j in range(len(temp_claim)):\n",
    "                claim = re.sub(r'<sub>(.*?)</sub>',\"\", str(temp_claim[j]))#subscript\n",
    "                claim = re.sub(r'<sup>(.*?)</sup>',\"\", str(claim))#subscript\n",
    "                claim = re.sub(r'(<.*?>)|(-->)|(\\\\)|\\bnum\\b', \" \", str(claim))\n",
    "                claim = re.sub(r'^\\s*?[0-9]+.\\s*', \"\", str(claim)).lower()\n",
    "                claim = tokenizer.tokenize(str(claim))\n",
    "                temp_claim[j] = claim\n",
    "                \n",
    "                #at word level\n",
    "                for ii in range(len(temp_claim[j])):\n",
    "                    if len(temp_claim[j][ii]) != 0:\n",
    "                        if temp_claim[j][ii][0] in list_num or temp_claim[j][ii] in stop_words :\n",
    "                            temp_claim[j][ii] = ''\n",
    "                        else:\n",
    "                            temp_claim[j][ii] = word_lemmatizer.lemmatize(temp_claim[j][ii], pos=\"v\")\n",
    "            claims.extend(temp_claim)\n",
    "                \n",
    "                \n",
    "    end = time.time()\n",
    "    print('time to complete the pre_processing_text_w2v', end - start)\n",
    "        \n",
    "    return abstracts, claims \n",
    "\n",
    "\n",
    "def text_doc(abstracts, claims, name):\n",
    "    \n",
    "    \"\"\"create a text file and write in it, where \n",
    "    each line is either an abstract or a claim\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    list_text = abstracts + claims\n",
    "    \n",
    "    #text_file=open('text_no_num_and_stop_word_epo.txt','w')\n",
    "    \n",
    "    with  open(ospath('~/code_final_project/C_Feature_extraction/Data_for_w2v/text_epo'+ name + '.txt'), 'w',  encoding=\"utf-8\") as text_file: \n",
    " \n",
    "        for i in range(len(list_text)):\n",
    "            doc = ' '.join(filter(None,list_text[i]))#filter and None help to elimate white spaces in the list of strings\n",
    "            if \" \" in doc and len(doc) != 0:#checks if the object is not empty and it is not just a string of numbers\n",
    "                text_file.write(doc + '\\n')\n",
    "        \n",
    "    end = time.time()\n",
    "    print('time to complete the text_doc function ', end - start)\n",
    "    \n",
    "    return print('done')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "open_and_save_text_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
