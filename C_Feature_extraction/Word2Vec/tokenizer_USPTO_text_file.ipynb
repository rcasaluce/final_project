{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import inflect\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from os.path import expanduser as ospath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_save_text_file(year):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    path = ospath('~/final_project/C_Feature_extraction/Data_for_w2v/USPTO/')\n",
    "    files = os.listdir(path)\n",
    "    claims, abstracts, = ([] for i in range(2))\n",
    "    for i in range(len(files)):\n",
    "        if files[i].endswith(str(year) +'.xlsx'):\n",
    "            print(files[i])\n",
    "            dataset = pd.read_excel(files[i], 'Sheet1',  index_col  = 0)\n",
    "            dataset = eliminate_na(dataset)\n",
    "            abstract, claim = pre_processing_text_w2v(dataset)\n",
    "            abstracts.extend(abstract)\n",
    "            claims.extend(claim)\n",
    "    text_doc(abstracts, claims, year)\n",
    "    end = time.time()\n",
    "    print('time to complete all the passages ', end - start)\n",
    "\n",
    "\n",
    "def pre_processing_text_w2v(dataset):\n",
    "    \n",
    "    \n",
    "    \"\"\"It processes the text\n",
    "    First, it stripes the text by the last tags and coverts each doc in lower case\n",
    "    Second, it eliminates the numbers in the text and converts the verbs to their root and it eliminates the stop_words\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    input = list of the text from both claims and abstracts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ouptup = abstracts and claims pre-processed\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+(?:[-\\\\]\\w+)?')#keeps words with hyphen and words back slash (problem it doubles the slash)\n",
    "\n",
    "    list_num = str(list((range(10))))\n",
    "    claims, abstracts, = ([] for i in range(2))\n",
    "    \n",
    "    print(dataset.shape[0])\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        print(i, end = ' ')\n",
    "\n",
    "        #parse abstracts\n",
    "        abstr = dataset[\"abstract\"].iloc[i]\n",
    "        abstr = re.sub(r'<.*?>','', str(abstr))#delete the last tags left in the text\n",
    "        abstr = re.sub(r'(&#x[a-z]*[0-9]+;)','', str(abstr))\n",
    "        abstr = re.sub(r'\\\\n','', str(abstr)).lower()\n",
    "        abstr = tokenizer.tokenize(str(abstr))\n",
    "        \n",
    "        for ii in range(len(abstr)):# to covert num to word\n",
    "            if len(abstr[ii]) != 0:\n",
    "                if abstr[ii][0] in list_num or len(abstr[ii]) <=2:\n",
    "                    #abstr[ii] = num_to_word(abstr[ii])\n",
    "                    abstr[ii] = ''\n",
    "        abstracts.append(abstr)\n",
    "\n",
    "        #parse claims\n",
    "        claim = (dataset[\"claims_text\"].iloc[i]).lower()\n",
    "        claim = claim.split(\"\\\\n\\\\n\\\\n\")\n",
    "        for j in range(len(claim)):\n",
    "            claim[j] = claim[j].replace('\\\\n', ' ')\n",
    "            claim[j] = re.sub(r'(&#x[a-z]*[0-9]+;)','', str(claim[j]))\n",
    "            claim[j] = tokenizer.tokenize(str(claim[j]))\n",
    "            claim[j] =  [x.lower() for x in claim[j]]\n",
    "            claim[j] = claim[j][1 :]#eliminates claim numbers\n",
    "            \n",
    "            for k in range(len(claim[j])):# to covert num to word\n",
    "                if len(claim[j][k]) !=0:\n",
    "                    if claim[j][k][0] in list_num or len(claim[j][k]) <=2:\n",
    "                        claim[j][k] = ''\n",
    "                   \n",
    "                    \n",
    "        claims.extend(claim)\n",
    "    end = time.time()\n",
    "    print('time to complete the pre_processing_text_w2v ', end - start)\n",
    "        \n",
    "    return abstracts, claims \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def text_doc(abstracts, claims, year):\n",
    "    \n",
    "    \"\"\"create a text file and write in it, where \n",
    "    each line is either an abstract or a claim\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    list_text = abstracts + claims\n",
    "    num_tokens = 0\n",
    "    \n",
    "    text_file = open(ospath('~/final_project/C_Feature_extraction/Data_for_w2v/USPTO/text_us'+ str(year) +'.txt'),'w')\n",
    "    \n",
    "    for i in range(len(list_text)):\n",
    "        doc = ' '.join(filter(None,list_text[i]))#filter and None help to elimate white spaces in the list of strings\n",
    "        num_tokens += len(doc)\n",
    "        if \" \" in doc and len(doc) !=0:#checks if the object is not empty and it is not just a string of numbers\n",
    "            text_file.write(doc + '\\n')\n",
    "    text_file.close()\n",
    "        \n",
    "    end = time.time()\n",
    "    print('time to complete the text_doc function ', end - start)\n",
    "    \n",
    "    return print('done',num_tokens)\n",
    "\n",
    "\n",
    "\n",
    "def eliminate_na(dataset):\n",
    "    \n",
    "    \"\"\"delete from each Excell files patents with \n",
    "    no abstract and just 1 claim\"\"\"\n",
    "    \n",
    "    dataset = dataset[(dataset['number_of_claims'] != 1) & (pd.notnull(dataset[\"abstract\"]))]\n",
    "    dataset.reset_index()\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "    \n",
    "read_and_save_text_file(2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
